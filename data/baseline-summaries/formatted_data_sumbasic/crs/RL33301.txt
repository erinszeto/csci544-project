program evaluations can play an important role in public policy debates and in oversight of government programs , potentially affecting decisions about program design , operation , and funding .

many different techniques of program evaluation can be used and presented with an intention to inform and influence policy makers .

one technique that has received significant recent attention in the federal government is the randomized controlled trial ( rct ) .

this report discusses what rcts are and identifies a number of issues regarding rcts that might arise when congress considers making program evaluation policy .

for example , in the 109 th congress , section 3 of s. 1934 ( as introduced ) would establish a priority for rcts when evaluating offender reentry demonstration projects ; section 114 of s. 667 ( senate finance committee - reported bill ) would require rcts for demonstration projects for low - income families ; and section 5 of s. 1129 ( as introduced ) would call for rcts for projects and policies of multilateral development banks .

issues regarding rcts could also arise when actors in the policy process present specific program evaluations to congress ( eg , in the president's budget proposals ) to influence congress's views and decision making .

for many reasons , evaluations often merit scrutiny and care in interpretation .

before discussing rcts in detail , the report places them in context by discussing ( 1 ) questions that program evaluations are typically intended to address , ( 2 ) how rcts relate to other program evaluation methods , and ( 3 ) two major roles that congress often takes with regard to program evaluation .

the report next describes the basic attributes of an rct , major ways to judge an rct's quality , and diverse views about the practical capabilities and limitations of rcts as a form of program evaluation .

in light of concerns about the reliability of individual studies to support decision making , the report also discusses how rcts can fit into systematic reviews of many evaluations .

the report next highlights two areas where rcts have garnered recent attention — in education policy and the president's annual budget proposal to congress .

finally , the report identifies potential issues for congress that could apply to the highlighted cases , oversight of other policy areas , and pending legislation .

because the vocabulary of program evaluation can be confusing , an appendix provides a glossary with definitions of selected terms .

the following two subsections briefly illustrate how rcts have been a subject of attention in two contexts: ( 1 ) setting program evaluation policy in one specific policy area ( education ) and ( 2 ) the citation and use of individual studies , or claimed lack thereof , to justify policy and budget proposals to congress ( in this case , as a component of the george w. bush administration's program assessment rating tool ) .

because this report's purpose is limited to providing an overview of rcts and related issues , these cases are not analyzed in detail in the report .

however , many of the issues identified in this report could be applied to these and other cases .

the previous section of this report illustrated how rcts have been subjects of prominent attention in two contexts: ( 1 ) setting program evaluation policy and ( 2 ) citation and use of individual studies , or lack thereof , to justify policy and budget proposals to congress .

in these and potentially other cases , a focus on rcts might raise multiple issues for congress .

some relate specifically to rct studies , including an rct's structural requirements and constraints .

other issues relate to program evaluation generally , and therefore to rcts .

a number of these issues are identified and analyzed below .

if congress wants to focus on rcts in the context of program evaluation policy ( eg , developing legislation for , or conducting oversight over , a program , agency , or the entire government ; or prospectively deciding whether to fund specific evaluations ) , congress might consider a number of issues related to the parameters of these studies and prospective risks to their internal and external validity .

in addition , issues could arise if congress wants to interpret or scrutinize individual rct studies ( eg , when they are presented to congress in the budget or authorization processes ) .

when making program evaluation policy , congress might opt to focus on some key parameters of studies , including random assignment , the cost of an rct , the length of time that an rct would take before producing findings , and privacy or ethical considerations .

as discussed earlier , the central attribute of an rct is the random assignment of subjects to treatment and control groups , which helps a researcher to make inferences that a particular intervention was responsible for an impact and to estimate that impact with reliable statistical tools .

in some programs , however , it may not be feasible or cost - effective to randomly assign units to an intervention group and a control group .

for example , it is not possible to conduct an rct on whether a policy regulating the release of chlorofluorocarbons into the environment contributes to overall global warming , because there is only one planet earth to study .

thus , if congress is considering whether to require certain types of evaluation for a program , or if congress is asked by an actor in the policy process to change funding for a program due to lack of experimental evidence of program impact , it might be important to question whether random assignment is possible or practical .

on the other hand , if a program would appear to allow for an evaluation using random assignment but none is planned , or an alternative is planned ( eg , a quasi - experiment ) , it might be important to consider whether an rct evaluation would be more appropriate .

for example , with respect to the cases discussed in this report , what programs assessed by the part are practically suitable for evaluation by an rct ? .

under the ed priority , why should quasi - experiments also receive a priority for funding , in addition to rcts ? .

if congress considers setting evaluation policy or directing specific studies for an agency or program , it might take the likely costs of an evaluation method or study into consideration , to weigh against the study's potential benefits .

large scale multi - site rcts are typically expensive , especially when the units being studied are not individual persons but rather organizations such as schools or jails .

large multi - site rcts of k - 12 education funded by ed have reportedly cost $10 million to $50 million .

in many cases , this level of funding is not available for the study of a federal program , due in part to tight budgetary constraints and the fact that program evaluations frequently must be paid for out of a program's budget .

smaller scale rcts featuring the random assignment of 100-200 individuals might be relatively inexpensive , reportedly costing from $300,000 to $700,000 .

even this cost , however , can often be as much as a studied program's funding level .

quasi - experiments are frequently , but not always , less expensive , but also might bring a different set of potential benefits and risks compared to an rct .

even if considerable funding is available for evaluations , pursuing a particular evaluation will leave fewer resources for other evaluation needs that might be judged important .

the opportunity cost of pursuing certain evaluations rather than others ( i.e. , cost associated with opportunities that are foregone by not putting resources to their highest value use ) can be high .

nevertheless , because the potential benefits and costs of evaluations can vary widely depending on an agency's or program's portfolio of evaluation needs , weighing these considerations can be difficult .

in addition , priorities might change depending on developments in an agency or policy environment .

in light of these and other considerations , congress might weigh the possible benefits of a study against the likely cost in view of the broader portfolio of evaluation needs , mindful of the risk that a study might be poorly designed or implemented or suffer from contamination that reduces confidence in the study's findings .

evaluation designs that do not offer the theoretical advantages of rcts regarding internal validity might , or might not , be worthwhile alternatives to rcts , depending on congress's evaluation objectives in specific situations .

rcts might take a long period of time to yield findings that can inform thinking and policy decisions .

for example , an rct that aims to estimate whether a certain aftercare program reduces the recidivism of juvenile offenders must follow the program's graduates , and the control group , over a multi - year time span .

this elongated time window might be problematic if policy decisions need to be made expeditiously .

however , the length of time necessary to conduct an rct ( and perhaps other complementary evaluations ) might justify waiting to make a policy decision until more evidence becomes available .

furthermore , programs or the external environment might have changed by the time the "old" program was evaluated .

how should the possible benefits of evaluations be weighed against risks of evaluation information becoming dated or obsolete ? .

what are the implications for the types and extent of evaluation research to be conducted ? .

congress could be called upon to consider these issues if it chose to establish program evaluation policy or directing and funding specific evaluations .

when considering whether to direct the use of rcts or other evaluations of public programs and policies , congress might consider whether the agencies conducting them are charged , or should be charged , with ethical duties to protect rct study participants' privacy , access to programs , and opportunity to give informed consent .

in addition , if congress determined that an agency should be charged with the ethical duties , congress might consider requiring oversight — by law , regulation , or institutional action — to help ensure that the duties were fulfilled .

privacy issues may arise in an rct if , for example , a program evaluation captures information about individual citizens or clients that could be used inside the government for a purpose other than for which it was collected , or released to the public in some form .

what , if any , safeguards should be required of those collecting the information ? .

several privacy protections are currently legally required for agency - conducted program evaluations and rcts by the privacy act ( 5 u.s.c .

§ 552a ) .

among other things , the act sets conditions concerning the disclosure of personally identifiable information , prescribes requirements for the accounting of certain disclosures of the information , requires agencies to specify their authority and purposes for collecting personally identifiable information from an individual , and provides civil and criminal enforcement arrangements .

if a program evaluation is funded or directed by an agency but conducted by a non - federal entity ( eg , if a non - federal entity creates and maintains records about program evaluation participants ) , the privacy act's coverage is often stated in contracts .

the issue of access to government programs might arise in an rct if , for example , the rct were designed with a control group that was to be denied access to a program as a part of the evaluation .

although access in some cases might not be required by law , its denial raises the question of whether the benefits of testing a program outweigh the burden of denying access to certain prospective subjects .

would it be appropriate to design rcts for entitlement programs , which guarantee services to clients , with such control groups ? .

the issue of informed consent might arise in an rct if it were deemed appropriate to enroll only willing participants .

although informed consent would not always be required by law for many rcts and other types of program evaluation , its use would guarantee that persons who participate in rcts fully understand and agree to their participation .

would the benefits of this outweigh burdens of the time and money that must be spent to achieve such a goal ? .

if congress chose to ensure that some or all of the above individual protections were implemented in rcts and other forms of program evaluation , it might consider requiring some form of protection - specific oversight .

although probably not required by law for rcts , the institutional review of a proposed research trial's protections for human participants is a common requirement for a great deal of research .

these reviews generally require that researchers obtain the approval of an institutional board prior to beginning the research .

the board checks to make certain that each proposal includes adequate protections for participants' privacy and ensures that the plan to obtain participants' informed consent is sufficient , among other things .

though this type of review may be time consuming and add additional costs to a research project , it can prove beneficial not only by protecting the study's participants , but also by incidentally improving a study's design .

congress and agencies have also instituted other oversight mechanisms for program evaluations , including competitions for grants and peer review of grant applications .

whether making program evaluation policy or scrutinizing studies , congress might also focus on issues of study interpretation and implementation ( eg , deciding whether to direct or fund evaluations in light of potential contamination risks and projected external validity , or judging how much confidence to put in the internal and external validity of a study presented during the budget or reauthorization processes ) .

actors in the policy process will not necessarily advertise any defects in the studies they present to influence congress .

with that in mind , when actors in the policy process present members or committees of congress with program evaluations intended to influence a policy decision , congress might consider the evaluation's realized , and not merely theoretical , internal validity .

in addition , when congress sets program evaluation policy for a given policy area , it might be possible to prospectively consider the probability of a study's successful design , implementation , and corresponding internal validity .

a major threat to the internal validity of rcts and quasi - experiments has been called contamination .

it should be noted that other designs that attempt to estimate impacts are subject to additional threats , including selection bias , as noted previously .

to avoid contamination , well - designed and implemented rcts ideally insulate the treatment and control groups from events that might affect one group during the study differently from the other group in a way that will affect outcomes .

doing so is intended to ensure the only systematic difference in the experience of the two groups is whether or not they received the intended treatment .

rcts also ideally ensure that the intended treatment was administered properly .

because social science research usually does not occur under tightly controlled laboratory conditions , however , it is often difficult to insulate a study from , or control for , unforeseen variables that might systematically affect the treatment and control groups differently .

three examples might help illustrate the threat of contamination .

first , if an experiment is not double - blinded , subjects in the treatment group might be aware of their inclusion in a special program .

if this awareness results in psychological effects that are not considered part of the treatment and subjects behave differently , the study's results might be contaminated .

similarly , some subjects in a control group might learn they are not in the treatment group and either decide to avail themselves on their own initiative of alternative treatments that are not associated with the intervention , or resent or undermine the treatment being given to the other group .

second , if a program to curb crime in a city were evaluated with an rct , certain districts might be the units of analysis .

perpetrators of crimes might move from experimental districts to control districts , contaminating findings for both groups .

third , with regard to treatment delivery , it might be difficult in some studies to ensure that the intended treatment is delivered properly for all subjects or all sites .

if some subjects in the control group get the treatment , for example , or if the intended treatment is not delivered properly , inferences about the intended intervention's impact might be contaminated .

in light of these considerations , several questions might be of concern .

when congress is presented with program evaluation findings , for example , what confidence should congress have that contamination did not degrade a study's internal validity ? .

does the study adequately address these risks ? .

also , when setting program evaluation policy , how much confidence should congress have that studies in certain policy areas or contexts will be able to avoid contamination ? .

what is the track record in a given area ? .

what are the implications for how congress and agencies should allocate scarce evaluation resources and structure an agency's portfolio of evaluations ? .

when congress is presented with a program evaluation to justify a policy position , the program that was evaluated presumably operated at a specific time and place , and under specific conditions .

without further analysis to gauge an evaluation's external validity , however , it will not always be clear whether the intervention itself or the study's findings can be generalized to other circumstances ( eg , future conditions , other subjects ) , as noted earlier in this report .

if generalizability were of concern , congress might in the first place consider whether the intervention itself ( i.e. , as it was actually implemented ) is replicable at a different time or place .

if the intervention was highly customized to a particular time or locale , for example , an evaluation's findings might not be generalizable elsewhere unless the intervention were fully replicated in all important respects .

alternatively , if an actual intervention were not clearly documented ( eg , how it operated , whom it served ) , it might be unclear how to replicate the intervention .

in that case , if the intervention were evaluated , it might not be reasonable to expect the evaluation's findings would be repeated elsewhere .

with regard to generalizability of findings ( as opposed to the intervention itself ) , if a single - site rct finds that an intervention had an impact for a group of subjects in one instance , it might not necessarily follow that it will have a similar impact for other subjects , times , and circumstances .

congress might therefore look for multiple impact analysis studies on the subject .

large scale , multi - site rcts are often considered more generalizable than single - site rcts , because they estimate an intervention's impact in a potentially wider array of geographic locations and populations .

however , multi - site rcts are also typically more expensive and difficult to successfully implement compared to single - site rcts .

in addition , complementary studies are often considered necessary to make assessments of generalizability .

complementary studies might include observational or qualitative evaluations , which can potentially be used to better understand an intervention's mechanism of causation , potential unintended consequences , and conditionality ( i.e. , the conditions that are required for the intervention to work as intended ) .

if these considerations were a source of concern , congress might scrutinize an evaluation's findings for external validity to other times , conditions , and subjects .

alternatively , if congress is setting evaluation policy for an agency or program , or is directing that specific studies occur , congress might provide direction or guidance regarding the evaluation methods that might be necessary for establishing a program's generalizability to other circumstances .

congress might also consider issues that apply to program evaluation generally .

because an rct is one of many types of program evaluation , these issues might be important when congress ( 1 ) considers making program evaluation policy for specific agencies or programs ( eg , what types of evaluations to direct or fund ) or ( 2 ) scrutinizes individual evaluations , including rcts , when making policy decisions and conducting oversight .

for example , if congress considers legislation that provides for program evaluation in one or more policy areas , to what extent should rcts be the focus of these evaluation policies ? .

to what extent should other evaluation methods be the focus ? .

should multiple methods fit into a broader evaluation framework ? .

the potential issues discussed below raise these and further questions .

given the nature of a policy area and the diverse needs of stakeholders — including agency program managers , the president , citizens , and notably congress — what different types of evaluations should be directed , funded , and considered ? .

this report began with a subsection titled " key questions about government programs and policies ," which outlined a number of questions that stakeholders often want to be informed about .

in response , a wide array of program evaluation techniques have been developed .

certain evaluation types often address , or help address , multiple kinds of questions .

indeed , many evaluation types are considered complementary to each other .

at the same time , the different types of evaluations bring their own sets of practical capabilities and limitations , and experts and practitioners sometimes disagree on the nature and importance of these capabilities and limitations .

because only finite resources are available for evaluation activities and staff , it can be challenging and controversial to determine the appropriate methods to be used to help answer certain stakeholder questions .

furthermore , when scrutinizing evaluations , it can be challenging to discern potential gaps in the perspectives provided by an actor in the policy process .

for example , it can be challenging to discern clues that might suggest other complementary evaluation types are needed .

given these considerations , many issues might be of congressional concern .

for example , in congress's view , how should the executive branch be pursuing evaluations under the part initiative , which particularly highlighted rcts ? .

how should congress oversee and respond to the administration's efforts to achieve "budget and performance integration" through use of the part ? .

in the education policy arena , to what extent is ed appropriately implementing the program evaluation aspects of nclb and esra ? .

to what extent is the ed priority , which elevated rcts and quasi - experiments above other evaluation types , consistent with congressional intent ? .

how is ed implementing the priority ? .

as congress considers legislation in other policy areas , should congress provide direction or guidance regarding program evaluation policy or methods ? .

if so , what kinds of studies might agencies , congress , and outside stakeholders need ? .

when actors in the policy process present evaluations to influence congress , are the actors presenting the full story , or are there gaps in the presentation ? .

are the evaluations they present capturing the key questions that need to be answered ? .

any of these multiple questions might be ripe for attention .

as noted earlier , many actors in the policy - making process use program evaluations to help justify their policy recommendations and to attempt to persuade congress to make decisions consistent with their policy objectives .

therefore , many observers have considered it important that policy makers , including members and committees of congress , be informed consumers of evaluation information .

unfortunately , however , the vocabulary of program evaluation can sometimes be confusing .

for example , many observers and practitioners use the same terms , but with differing definitions for those terms .

when someone says a program is "effective," in what sense is the term being used ? .

as noted previously , the term effectiveness might refer to ( 1 ) a program's overall merit or worth , ( 2 ) the extent to which a program is accomplishing its goals , ( 3 ) the program's impact on a particular outcome of interest , or ( 4 ) an ambiguous mix of all three prior definitions .

these are very different concepts .

in any of these senses , determining whether a program is "effective" can hinge upon an observer's views and assumptions about the program's mission , objectives , appropriate outcome ( s ) of interest , and progress .

thus , should these definitional aspects of program evaluations be of concern , congress might scrutinize them closely .

furthermore , assumptions that are implicit in an evaluation might go unstated .

some stakeholders or evaluators might implicitly argue that their preferred way of evaluating a program is "best" and that other methods are comparatively inappropriate or less appropriate .

however , there is not always consensus among well - respected experts regarding when certain methods are best or most appropriate .

should these matters be of concern to congress when they occur , congress might investigate a number of questions .

for example , if one method is claimed to be "best" compared to others , what are the stated and unstated reasons for that opinion ? .

what would other stakeholders and evaluators say ? .

to what extent , if any , might opinions of appropriateness be due to an underlying agenda ( eg , to support policy views ) or self - interest ( eg , to get funding for a program or a type of evaluation ) ? .

these questions might also be applied to the cases discussed in this report .

for example , in justifying the ed priority for rcts , which claimed rcts are "best" for determining "effectiveness," which definition for effectiveness is ed using ? .

is the ed definition consistent with how ed intends to use rcts ? .

should ed employ additional or alternative types of evaluation for these intended uses of rcts ? .

in its strategic planning , budgeting , and operations , is ed using multiple definitions ? .

what definition is being used for overall effectiveness , for purposes of the part ? .

when congress is presented with evaluation information in the policy process by various actors ( eg , lobbyists , experts , think tanks , academics , or agencies , among others ) , how should congress use the information and findings ? .

there is widespread consensus that program evaluations can help policy makers gain insights into policy problems and make better - informed decisions regarding ways to improve government performance , transparency , accountability , and efficiency .

nonetheless , the use of evaluation in the policy - making process can be controversial .

for example , some advocates of performance - based budgeting and evidence - based policy have argued that a program's future funding or existence should be "based" on its "performance" or on "evidence" of its "effectiveness. .

the terms performance - based budgeting and evidence - based policy , however , do not have consensus definitions , because different actors typically have , among other things , different definitions for what constitutes "performance" and "evidence," conceptions of what it means to "base" decision making on performance or evidence , views about whether a decision should be "based" on past performance or evidence , and views about what other factors should legitimately help drive decision making .

another fundamental question might be of concern to policy makers and stakeholders .

what role should evaluation of past events play in forming future strategies and plans ? .

a prominent argument in favor of using evaluations to shape future strategies is that past performance ( by a person , program , agency , etc. ) .

is usually the best predictor of future performance .

however , a prominent counter - argument is that focusing primarily on the past can be compared to "driving a car using only the rearview mirror. .

although evaluation of past performance is widely considered helpful for informing thinking and decisions , the process of strategic decision making has been found in the social science and management literatures to be legitimately driven by many more factors .

these have included , among others , basic and applied research , forecasting and scenario planning , risk assessment , professional judgment from individual and group experience , theoretical extrapolation , intuition ( especially when information is incomplete , consensus interpretations of information are lacking , the future is uncertain , or synthesis is necessary ) , and values .

in addition , in spite of efforts to make decisions as rational as possible in the face of uncertainty and limited information , a wide body of social science has found that there are practical limits to rationality in decision making .

should these considerations be of concern to congress when considering policy questions or conducting oversight , several questions might be asked .

for example , in an rct context , when considering or scrutinizing rct studies , how should these studies be used by congress to inform thinking and decision making ? .

how should they be used by agencies and omb ? .

are agencies and omb using them in appropriate ways ? .

what other factors can or should be considered ? .

if congress is presented in a decision - making situation with a single program evaluation , is the evaluation enough to have high confidence in the findings ? .

program evaluations can provide helpful insight into policy problems and the manner and extent to which federal policies address those problems .

unfortunately , however , one or more program evaluations do not always produce information that is comprehensive , accurate , credible , or unbiased .

for example , a program evaluation can be designed to answer a certain question , but the way someone frames the original evaluation question can influence how a program is ultimately portrayed .

most prominently , this can be the case when setting criteria for "success," such as a program's goals or the preferred outcomes of interest .

however , actors in the policy process often have varying views on how to judge a program's or policy's success .

it is not always clear , therefore , that a study's research question will be viewed by most observers as covering what should have been covered to validly or comprehensively evaluate a program .

in addition , a program evaluation will not always necessarily be well designed or implemented .

in such cases , a study might produce results that are flawed or inaccurate .

even in the best case — if an evaluation is appropriate for the research question being studied , is well designed and implemented , and there is widespread consensus on how to judge "success" — it is still possible that random chance or unforeseen events might result in an evaluation that produces information that is inaccurate or flawed .

for example , it is possible that an evaluation might provide a "false positive" or "false negative" result ( eg , the study finds the program successful when actually it was not , or finds a program unsuccessful when it actually was successful ) .

if this situation were the case , the study findings might not reveal it .

the subject of data or information quality is also oftentimes a subject of concern when conducting or interpreting program evaluations .

how might congress cope with these possibilities ? .

how confident must a member or committee be in evaluation information , including from rcts , in order to use the information to inform thinking and conclusions about a policy ? .

in response , social science researchers have recommended that consumers of evaluation information be aware of the practical capabilities and limitations of various program evaluation methods and also scrutinize a study's claims of internal , external , and construct validity .

they have also suggested looking for multiple studies and , if available , systematic reviews .

other observers have suggested using the resources of gao or other congressional support agencies to help interpret or validate conclusions and scrutinizing these matters through hearings and oversight .

finally , congress might consider whether federal agencies have sufficient capacity and independence to conduct , interpret , and objectively present program evaluations to congress .

at times , congress and other actors have expressed concern over the capacity of agencies to adequately perform certain tasks , including management functions that range from procurement to financial management .

one management function that has been frequently cited as a topic of concern is program evaluation .

over a long period of time , gao has "found limited ( and diminishing ) resources spent on ... program evaluation" and "reason to be concerned about the capacity of federal agencies to produce evaluations of their programs' effectiveness. .

even with recent emphasis on program evaluation under gpra and the bush administration's part , it is unclear the extent to which agencies have capacity to properly conduct , interpret , or use program evaluations .

many , if not all , of the issues discussed in this report could apply equally to organizations and decision makers within federal agencies .

should program evaluation capacity in federal agencies be seen as a topic of concern , several questions might be considered by congress .

given the complex issues and debates involved in the production , interpretation , and use of program evaluations — as well as complex debates about the appropriateness of different evaluation types in certain circumstances — do agencies have capacity to use evaluation information to soundly inform strategic and operational decisions ? .

in addition , do agencies have the capacity to make objective , methodologically sound presentations and interpretations of evaluation information to congress , including information from rcts ? .

furthermore , do agency program evaluation offices and personnel have the necessary independence from politics ( eg , partisan or institutional ) and self - interest to , without undue hindrance , raise potentially uncomfortable issues and surface objective , valid , and reliable findings for consideration by policy makers , including congress ? .

should they have this kind of independence ? .

finally , and more broadly , congress might consider issues of evaluation capacity that go beyond federal programs .

in the past , congress has established agencies that focus on evaluation issues in entire policy areas .

for example , in 1989 , congress established a new agency within the department of health and human services to serve as a focal point in health care research .

congress reauthorized the agency , now called the agency for healthcare research and quality ( ahrq ) , in 1999 .

rather than focus only on evaluating federal programs , ahrq's statutory mission is to conduct and support research in all aspects of health care , synthesis and dissemination of available scientific evidence for use by multiple stakeholders , and initiatives to advance health care quality .

as noted previously in this report , congress also established ies within ed in 2002 .

ies has a multi - part mission "to provide national leadership in expanding fundamental knowledge and understanding in education from early childhood though post - secondary study" for many stakeholders , providing them with reliable information about "the condition and progress of education in the united states...," "educational practices that support learning and improve academic achievement and access to educational opportunities for all students," and "the effectiveness of federal and other education programs" ( 116 stat .

1944 ) .

although there are differences in the missions of these agencies , one aspect that arguably makes them similar is the scope of their research and evaluation work .

specifically , their focus goes beyond federal programs to instead encompass research and evaluations throughout an entire policy area such as "education" or "health care," whether interventions are delivered by the federal government or another entity .

should congress view program evaluation capacity as an issue for an entire policy area , congress could move to consider whether establishment of a policy research and evaluation agency might be warranted .

the vocabulary of program evaluation unfortunately for consumers of evaluation information , the vocabulary of program evaluation can sometimes be technical and difficult .

the field is multi - disciplinary and some concepts are complex .

sometimes it is not always clear in what sense a term is being used and whether the term is being used appropriately .

technical experts and actors in the policy process sometimes use the same terms for different concepts or use different terms for the same concept .

nonetheless , in program evaluations , understanding the meanings of and distinctions between key terms can make a significant difference in how to interpret study findings and limitations and in how to scrutinize evaluations to see if they are being represented objectively and forthrightly .

this appendix draws on the report to briefly define and , if necessary , explain several recurring terms and to briefly identify several definitions for the same term , as appropriate .

however , the definitions provided below are illustrative only and do not necessarily indicate what an evaluation's author or what an actor in the policy process intends to communicate .

more definitive assessments typically must be made on a case - by - case basis .

the footnotes in this report provide written resources that can help with understanding evaluations and the terms they employ , and crs analysts can provide additional assistance or refer readers to other resources .

in each entry , a term that is included elsewhere in the glossary is written in italics the first time it is used .

selected terms and concepts construct validity in practice , there are several definitions of this term: ( 1 ) in measuring outcomes , the extent to which a study actually evaluates what it is being represented as evaluating ( eg , does the study's outcome of interest actually measure "student achievement" ? .

 ) ; and ( 2 ) in relation to a program , the extent to which the actual program reflects one's ideas and theories of how the program is supposed to operate , and the causal mechanism through which it is supposed to achieve outcomes .

contamination in an rct , something aside from the intended treatment that might affect the treatment group or control group differently from the other group in a way that will affect observed outcomes .

for example , rcts should ideally insulate the treatment and control groups from contaminating events in order to ensure that the only difference in the experience of the two groups is whether or not they received the intended treatment .

in addition , rcts should ideally ensure that the treatment was administered properly ; otherwise , the treatment might be considered contaminated .

control group in an rct , a group of subjects chosen by random assignment that is comparable to the treatment group but that does not experience the program being studied .

effect depending on usage , something that inevitably follows an antecedent ( as a cause or agent ) ; for example , the act of dropping a pen — a cause — is closely followed by a noise — an effect — when the pen strikes the floor .

also used sometimes as a synonym for impact .

effective ( effectiveness ) a term with multiple possible definitions .

in practice , it is used as a synonym for impact , merit and worth , or accomplishment of specific , intended goals .

evaluation an applied inquiry process for collecting and synthesizing evidence that culminates in conclusions about the state of affairs , value , merit , worth , significance , or quality of a program , product , person , policy , proposal , or plan .

external validity the extent to which an intervention being studied can be ( 1 ) applied to and replicated in other settings , times , or groups of subjects and ( 2 ) expected to deliver a similar impact on an outcome of interest .

the terms generalizability , replicability , and repeatability are sometimes used as synonyms for external validity .

some usage of this term refers only to one of the two aspects noted here .

government performance and results act ( gpra ) of 1993 a federal law that requires most executive branch agencies to develop five - year strategic plans , annual performance plans ( including goals and performance indicators , among other things ) , and annual program performance reports .

in gpra's legislative history , it was contemplated that not all forms of program evaluation and measurement would necessarily be quantifiable because of the large diversity of federal government activities .

impact an estimated measurement of how a program intervention affected the outcome of interest for a large group of subjects , on average , compared to what would have happened without the intervention .

for example , if the unemployment rate in a geographic area would have been 6% without an intervention , but was estimated to be 5% because of the intervention , the impact would be a 1% reduction in the unemployment rate ( i.e. , 6% minus 5% equals an impact of 1% ) , or , alternatively , a 16.7% reduction in the unemployment rate , if one characterizes the impact as a proportion of the prior unemployment rate .

depending on the chosen outcome of interest , the average impact across all subjects usually reflects the weighted average of the subjects who experienced favorable impacts , subjects who did not experience a change , and others who experienced unfavorable impacts .

some theorists and practitioners use the term effect as a synonym for impact .

internal validity in an rct , the confidence with which one can state that the impact found or inferred by a study was caused by the intervention being studied .

merit the overall intrinsic value of a program to individuals .

this term is usually paired with the term worth .

meta - analysis a type of systematic review that uses statistical methods to derive quantitative results from the analysis of multiple sources of quantitative evidence .

observational design a term that has been used in different ways , but that often refers to empirical and qualitative evaluations of many types that are intended to help explain cause - and - effect relationships but do not attempt to approximate an rct .

outcome of interest something , oftentimes a public policy goal , that one or more stakeholders care about ( eg , unemployment rate , which many actors might like to be lower ) .

there can be many potential outcomes of interest related to a program .

actors in the policy process will not necessarily agree which outcomes are important .

outcomes of government programs need not always be quantitative ( eg , sending humans safely to the moon and back to earth ) .

performance measurement ( performance measure ) a term that can mean many things but is usually considered to be different from program evaluation .

typically , the term refers to ongoing and periodic monitoring and reporting of program operations or accomplishments ( eg , progress toward quantitative goals ) and sometimes also statistical information related to , but not necessarily influenceable by , a program .

occasional synonyms for "measure" are indicator , metric , and target .

sometimes the word "performance" is dropped , especially when stakeholders believe the measure does not necessarily indicate whether the program itself caused changes in favorable or unfavorable directions .

program a government policy , activity , project , initiative , law , tax provision , function , or set thereof , that someone might wish to evaluate .

in program evaluation , synonyms for program include treatment and intervention .

program evaluation under the government performance and results act ( gpra ) of 1993 , "an assessment , through objective measurement and systematic analysis , of the manner and extent to which federal programs achieve intended objectives. .

program evaluation has been seen as ( 1 ) informing conclusions at particular points in time and also ( 2 ) a cumulative process over time of forming conclusions , as more evaluation information is collected and interpreted .

practitioners and theorists categorize different types of program evaluation in several ways .

the varying types are sometimes referred to generically as designs or methods .

typical synonyms for this term include evaluation and study .

qualitative evaluation a wide variety of evaluation types that judge the effectiveness of a program ( eg , whether it accomplishes its goals ) by , among other things , conducting open - ended interviews , directly observing program implementation and outcomes , reviewing documents , and constructing case studies .

quasi - experimental design a type of evaluation that attempts to estimate a treatment's impact on an outcome of interest for a group of subjects but , in contrast with rcts , does not have random assignment to treatment and control groups .

some quasi - experimental designs are controlled studies ( i.e , with a control group and at least one treatment group ) , but others lack a control group .

some quasi - experiments do not measure the outcome of interest before the treatment takes place .

some observers and practitioners consider quasi - experiments to be a form of observational design , but others put them in their own category .

random assignment the process of assigning subjects into a control group and one or more treatment groups by random chance .

random selection the process of drawing a sample by random chance from a larger population ( eg , to undertake a survey that is intended to be representative of a broader population ) .

this term is different from , and sometimes confused with , random assignment .

randomized controlled trial ( rct ) in its basic form , an evaluation design that uses random assignment to assign some subjects to a treatment group and also to a control group .

the treatment group participates in the program being evaluated and the control group does not .

after the treatment group experiences the intervention , an rct attempts to compare what happens to the two groups , as measured by the resulting difference between the two groups on the outcome of interest , in order to estimate the program's impact .

the terms randomized field trial ( rft ) , random assignment design , experimental design , random experiment , and social experiment are sometimes used as synonyms for rct , and vice versa .

use of the word "field" in this context is often intended to imply that an evaluation is being conducted in a more naturalistic setting instead of a laboratory or other artificial environment .

double - blind studies are those in which neither the subjects nor the researchers know which group gets the treatment .

single - blind studies are those in which the subjects do not know they are getting the treatment being investigated .

statistical significance in the context of an rct , a finding of statistical significance is typically interpreted as a level of confidence ( usually expressed as a probability , eg , 95% , which is also referred to as "significance at the .05 level" ) that an estimated impact is not merely the result of random variation .

assuming the rct suffered from no defects , this finding would indicate that at least some of the measured impact may with substantial confidence ( eg , 95% confidence ) be attributed to the treatment as a cause .

stated another way , significance at the .05 level indicates that there is a 1 in 20 chance that the observed difference could have occurred by chance , if the program actually had no impact .

however , simply because an estimated impact is found to be statistically significant does not necessarily mean the impact is large or important .

systematic review a form of structured literature review that addresses a question that is formulated to be answered by analysis of evidence , and involves objective means of searching the literature , applying predetermined inclusion and exclusion criteria to this literature , critically appraising the relevant literature , and extraction and synthesis of data from the evidence base to formulate findings .

although systematic reviews typically focus much attention on concerns about internal validity of various studies , judgments about external validity , or generalizability of findings , are often left to readers to assess , based on their implicit or explicit decision how applicable the systematic review's evidence is to their particular circumstances .

in program evaluation , systematic reviews have been performed under various names ( eg , evaluation synthesis , integrative review , research synthesis ) , in different ways , and usually in decentralized fashion .

some systematic reviews focus on rcts ( and might include quasi - experiments ) , and others include disparate types of studies .

treatment group in an rct , the group of subjects chosen by random assignment that experiences or participates in a program ; also sometimes called an experimental or intervention group .

unit of analysis in an rct , the subjects of the study who are randomly assigned to one or more treatment groups and also a control group .

subjects are typically individual persons but sometimes might be things or organizations like schools , hospitals , or police stations .

validity see entries for internal validity , external validity , and construct validity .

worth the overall extrinsic value of a program to society .

this term is usually paired with the term merit .

